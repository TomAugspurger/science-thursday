{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalable Machine Learning with Dask-ML\n",
    "\n",
    "*https://coiled.io/blog/scalable-machine-learning.html*\n",
    "\n",
    "Standard disclaimer: Python has great tools for single-node machine learning.\n",
    "Distributed computing is fundamentally harder, so think twice before reaching for\n",
    "Dask-ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "cluster = coiled.Cluster(n_workers=10)\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When *might* you need scalable / distributed ML?\n",
    "\n",
    "1. You have a compute-bound problem. Slow training time is affecting your workflow.\n",
    "2. You have a memory-bound problem *and* more data helps.\n",
    "\n",
    "## Dimensions of Scale\n",
    "\n",
    "![](dimensions.png)\n",
    "\n",
    "## 1. CPU (Compute)-Bound Problems\n",
    "\n",
    "In this type of problem, your dataset fits in RAM just fine, but you're waiting around for your CPU (or GPU, TPU) to finish it's computations. This commonly occurs when there's many mostly independent components to your estimator\n",
    "\n",
    "* Hyperparameter Optimization: Many CV-splits / hyperparameter combinations\n",
    "* Ensemble estimator: Combine predictions from many estimators\n",
    "\n",
    "These are relatively straightforward to parallelize. Here's a small example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X, y = make_classification(n_samples=5_000, random_state=0)\n",
    "X[:5]\n",
    "\n",
    "param_grid = {\"C\": [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0],\n",
    "              \"kernel\": ['rbf', 'poly', 'sigmoid'],\n",
    "              \"shrinking\": [True, False]}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(gamma='auto', random_state=0, probability=True),\n",
    "                           param_grid=param_grid,\n",
    "                           return_train_score=False,\n",
    "                           cv=5,\n",
    "                           n_jobs=-1)\n",
    "grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a single estimator takes 1-2 seconds on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time grid_search.estimator.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit the whole grid search on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import joblib\n",
    "\n",
    "with joblib.parallel_backend('dask', scatter=[X, y]):\n",
    "    grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Happened?\n",
    "\n",
    "Internally, scikit-learn parallelizes `for` loops with joblib. Typically that parallel `for` loop uses threads or processes on a single machine. We worked with the scikit-learn / joblib devs to implement a `dask` parallel backend, so you can parallelize things on a cluster.\n",
    "\n",
    "<img src=\"joblib.png\" width=\"100%\"/>\n",
    "\n",
    "<img src=\"joblib-dask.png\" width=\"100%\"/>\n",
    "\n",
    "Pretty much anything that uses joblib internally can use the Dask joblib backend.\n",
    "\n",
    "* Anything in scikit-learn with `n_jobs` (fitting trees in a Random Forest, voting methods, hyperparamter optimization, ...)\n",
    "* TPOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Example\n",
    "\n",
    "Let's apply the joblib parallelization to a more realistic example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv(\n",
    "    \"s3://nyc-tlc/trip data/yellow_tripdata_2019-*.csv\",\n",
    "    dtype={\n",
    "        \"payment_type\": \"UInt8\",\n",
    "        \"VendorID\": \"UInt8\",\n",
    "        \"passenger_count\": \"UInt8\",\n",
    "        \"RatecodeID\": \"UInt8\",\n",
    "    },\n",
    "    parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"],\n",
    "    blocksize=\"16 MiB\",\n",
    "    storage_options=dict(anon=True),\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = df.sample(frac=0.001, random_state=0).compute()\n",
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.dropna()\n",
    "len(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vendor_dtype = pd.CategoricalDtype([1, 2, 4])\n",
    "ratecode_dtype = pd.CategoricalDtype([1, 2, 3, 4, 5, 6, 99])\n",
    "store_and_fwd_flag = pd.CategoricalDtype([\"N\", \"Y\"])\n",
    "payment_type = pd.CategoricalDtype([1, 2, 3, 4, 5])\n",
    "\n",
    "dtypes = {\n",
    "    \"VendorID\": vendor_dtype,\n",
    "    \"RatecodeID\": ratecode_dtype,\n",
    "    \"store_and_fwd_flag\": store_and_fwd_flag,\n",
    "    \"payment_type\": payment_type,\n",
    "    \"passenger_count\": \"int\",\n",
    "}\n",
    "sdf = sdf.astype(dtypes)\n",
    "X = sdf.drop([\"tip_amount\", \"total_amount\"], axis=\"columns\")\n",
    "y = sdf[\"tip_amount\"] > 0\n",
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.pipeline\n",
    "import sklearn.compose\n",
    "import sklearn.preprocessing\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "import sklearn.ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_datetime(X):\n",
    "    out = []\n",
    "    for k, v in X.items():\n",
    "        out.append(v.dt.dayofweek.rename(f\"{k}_dow\"))\n",
    "        out.append(v.dt.hour.rename(f\"{k}_hour\"))\n",
    "\n",
    "    return pd.concat(out, axis=\"columns\")\n",
    "\n",
    "def transform_location(X):    \n",
    "    return (X[\"PULocationID\"] == X[\"DOLocationID\"]).to_frame(name=\"same_location\")\n",
    "\n",
    "\n",
    "# Dummy encode the categorical columns\n",
    "onehot_columns = list(sdf.select_dtypes(include=\"category\"))\n",
    "onehot_categories = [sdf[col].dtype.categories for col in onehot_columns]\n",
    "ohe = sklearn.preprocessing.OneHotEncoder(categories=onehot_categories, sparse=False)\n",
    "\n",
    "# Datetime & Dummy encode the datetime columns\n",
    "datetime_columns = list(sdf.select_dtypes(include=\"datetime\"))\n",
    "dte = sklearn.pipeline.make_pipeline(\n",
    "    sklearn.preprocessing.FunctionTransformer(transform_datetime),\n",
    "    sklearn.preprocessing.OneHotEncoder( \n",
    "        categories=[\n",
    "            list(range(7)),  # day of week\n",
    "            list(range(24))  # hour\n",
    "        ] * len(datetime_columns),\n",
    "        sparse=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Location encode location IDs\n",
    "location_columns = [\"PULocationID\", \"DOLocationID\"]\n",
    "le = sklearn.preprocessing.FunctionTransformer(transform_location)\n",
    "\n",
    "# Compose the pipeline\n",
    "preprocess = sklearn.compose.make_column_transformer(\n",
    "    (ohe, onehot_columns),\n",
    "    (dte, datetime_columns),\n",
    "    (le, location_columns),\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "scale = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "pipe = sklearn.pipeline.make_pipeline(\n",
    "    preprocess,\n",
    "    scale,\n",
    "    sklearn.ensemble.GradientBoostingClassifier(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.set_config(display=\"diagram\")\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_ml.model_selection\n",
    "\n",
    "param_grid = {\n",
    "    \"gradientboostingclassifier__learning_rate\": [0.001, 0.01, .1, 1],\n",
    "    \"gradientboostingclassifier__max_leaf_nodes\": [10, 31, 50],\n",
    "}\n",
    "search = dask_ml.model_selection.GridSearchCV(pipe, param_grid)\n",
    "search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = search.fit(X[:50_000], y[:50_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory-Bound Problems\n",
    "\n",
    "![](dimensions.png)\n",
    "\n",
    "Dask-ML also has tools for working with larger-than-memory datasets. We can break this scaling challenge into a couple components\n",
    "\n",
    "1. Data structures: NumPy & pandas were built for in-memory problems.\n",
    "2. ML Algorithms: Many algorithms in (e.g.) Scikit-Learn were built for (in-memory) NumPy arrays.\n",
    "\n",
    "But first, plot your learning rate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a very simple / fast to train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "\n",
    "lr = sklearn.linear_model.LogisticRegression(max_iter=500)\n",
    "pipe = sklearn.pipeline.make_pipeline(preprocess, scale, lr)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use scikit-learn's built-in [`learning_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html) to measure how the model does on various sample sizes (all of which fit in memory). It uses `joblib` internally, so we can use Dask to do the learning curve in parallel on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = X[:50_000]\n",
    "yy = y[:50_000]\n",
    "\n",
    "with joblib.parallel_backend(\"dask\", scatter=[xx, yy]):\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = (\n",
    "        sklearn.model_selection.learning_curve(pipe, xx, yy,\n",
    "                                               return_times=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "fit_times_mean = np.mean(fit_times, axis=1)\n",
    "fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, figsize=(6, 15))\n",
    "# Plot learning curve\n",
    "axes[0].grid()\n",
    "axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                     color=\"g\")\n",
    "axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "axes[0].legend(loc=\"best\")\n",
    "\n",
    "# Plot n_samples vs fit_times\n",
    "axes[1].grid()\n",
    "axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
    "axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "                     fit_times_mean + fit_times_std, alpha=0.1)\n",
    "axes[1].set_xlabel(\"Training examples\")\n",
    "axes[1].set_ylabel(\"fit_times\")\n",
    "axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "# Plot fit_time vs score\n",
    "axes[2].grid()\n",
    "axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
    "axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1)\n",
    "axes[2].set_xlabel(\"fit_times\")\n",
    "axes[2].set_ylabel(\"Score\")\n",
    "axes[2].set_title(\"Performance of the model\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this model overfits at 5,000 samples, but by 15,000 samples has essentially plateaued. Throwing more data at this model isn't really going to improve your fit.\n",
    "\n",
    "But assuming it did, how would we scale our earlier pipeline? The first issue is that we can't fit it in a NumPy array / pandas DataFrame. So we (the user and Dask-ML) use Dask Array and Dask DataFrame instead.\n",
    "\n",
    "```python\n",
    ">>> df = dd.read_csv(...)    # Instead of pandas\n",
    ">>> arr = da.from_zarr(...)  # Instead of NumPy\n",
    "```\n",
    "\n",
    "The second issue was around ML algorithms. Depending on the algorithm, implementing a parallel / distributed version can be difficult (or impossible). Many preprocessing tasks are doable.\n",
    "\n",
    "* [`dask_ml.preprocessing`](https://ml.dask.org/preprocessing.html)\n",
    "  - MinMaxScaler\n",
    "  - QuantileTransformer\n",
    "  - LabelEncoder\n",
    "  - OneHotEncoder\n",
    "  - ...\n",
    "* [`dask_ml.feature_extraction`](https://ml.dask.org/modules/api.html#dask-ml-feature-extraction-text-feature-extraction)\n",
    "  - CountVectorizer\n",
    "  - HashingVectorizer\n",
    "  - FeatureHasher\n",
    "  \n",
    "On the other hand, things like a parallel, distributed SGD or gradient boosted tree are feasible, but a lot of work to implement and maintain.\n",
    "\n",
    "We'll often continue to re-use scikit-learn & others by using the `partial_fit` API. We feed blocks of a Dask Array / DataFrame to scikit-learn. Since these are just NumPy arrays or pandas DataFrames, scikit-learn already knows what to do. And it can incrementally learn the parameters. See https://ml.dask.org/incremental.html for more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blockwise Ensemble Method\n",
    "\n",
    "Dask-ML recently added blockwise ensemble voting estimators. They're an interesting way to combine the best of Dask and existing ML libraries like scikit-learn.\n",
    "\n",
    "Let's generate a big dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask_ml.datasets\n",
    "\n",
    "X, y = dask_ml.datasets.make_classification(n_samples=10_000_000,\n",
    "                                            n_features=1000,\n",
    "                                            n_informative=10,\n",
    "                                            shift=2, scale=2,\n",
    "                                            chunks=50_000)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll persist it in memory on the cluster, and bring back a small sample to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dask.persist(X, y)\n",
    "xx, yy = dask.compute(X[:1000], y[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take some estimator like `RidgeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subestimator = sklearn.linear_model.RidgeClassifier()\n",
    "subestimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time subestimator.fit(xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this estimator to the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = subestimator.predict(xx)\n",
    "X.map_blocks(subestimator.predict, drop_axis=1, meta=meta)[::1000].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This estimator has only seen a bit of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import dask_ml.ensemble\n",
    "\n",
    "subestimator = sklearn.linear_model.RidgeClassifier(random_state=0)\n",
    "clf = dask_ml.ensemble.BlockwiseVotingClassifier(\n",
    "    subestimator,\n",
    "    classes=[0, 1]\n",
    ")\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = clf.predict(X)\n",
    "yhat[::100].compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
